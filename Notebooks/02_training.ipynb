{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ed226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create traing config\n",
    "# initialize config file \n",
    "# !python -m spacy init config --lang en --pipeline spancat E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs_spancat\\config_spancat_trf_acc.cfg --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae090d",
   "metadata": {},
   "source": [
    "# Approaching as spacy NER problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860148f7",
   "metadata": {},
   "source": [
    "## NER with conventional backend\n",
    "#### config_ner_ef is made for ner and efficiency with conventinal spacy backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67840c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved config\u001b[0m\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_1.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_1.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# auto fill the config_ner_ef file\n",
    "!python -m spacy init fill-config E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_ef.cfg E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_ef.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25f3ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory:\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_ef\u001b[0m\n",
      "\u001b[38;5;4m[i] Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     54.50    5.33    3.32   13.57    0.05\n",
      "  0      10          0.80    459.42    0.00    0.00    0.00    0.00\n",
      "  0      20          1.15    225.16    8.47   43.02    4.70    0.08\n",
      "  0      30          0.80    228.88   18.43   36.22   12.36    0.18\n",
      "  0      40          0.44    146.27   32.34   33.50   31.26    0.32\n",
      "  0      50          0.72    135.99   36.54   39.00   34.38    0.37\n",
      "  0      60          0.98    158.28   27.17   26.37   28.03    0.27\n",
      "  0      70          0.66    125.73   29.73   34.99   25.84    0.30\n",
      "  0      80          0.62    116.30   37.95   42.46   34.30    0.38\n",
      "  0      90          0.71    125.38   43.83   46.65   41.34    0.44\n",
      "  0     100          0.74    128.67   43.27   43.12   43.43    0.43\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_ef\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-07 12:18:55,031] [INFO] Set up nlp object from config\n",
      "[2023-03-07 12:18:55,047] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2023-03-07 12:18:55,051] [INFO] Created vocabulary\n",
      "[2023-03-07 12:18:55,053] [INFO] Finished initializing nlp object\n",
      "[2023-03-07 12:19:03,462] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# train spacy\n",
    "!python -m spacy train E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_ef.cfg --output E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_ef\\ --paths.train E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\data\\Train_ner\\train\\train.spacy --paths.dev E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\data\\Train_ner\\dev\\dev.spacy --training.eval_frequency 10 --training.max_steps 100 --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a74f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy debug config E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_1.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9b7f1",
   "metadata": {},
   "source": [
    "## NER with transformer\n",
    "#### config_ner_trf_acc is made for ner and accuracy with transformer backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86759a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved config\u001b[0m\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_2.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_2.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# auto fill the config_2 file\n",
    "!python -m spacy init fill-config E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_trf_acc.cfg E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_trf_acc.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0858bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory:\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_trf_acc\u001b[0m\n",
      "\u001b[38;5;4m[i] Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0          88.77    191.70    0.06    0.28    0.03    0.00\n",
      "  0      10        9449.07  16287.68    0.00    0.00    0.00    0.00\n",
      "  0      20        6015.67  12512.76    0.00    0.00    0.00    0.00\n",
      "  0      30        4921.33  10464.82    0.00    0.00    0.00    0.00\n",
      "  0      40        2282.44   4740.72    0.00    0.00    0.00    0.00\n",
      "  0      50        1829.62   3676.05    0.00    0.00    0.00    0.00\n",
      "  0      60        1522.88   3002.58    0.00    0.00    0.00    0.00\n",
      "  0      70        1469.58   2842.80   33.27   40.13   28.42    0.33\n",
      "  0      80        1214.33   2333.63   54.09   54.00   54.17    0.54\n",
      "  0      90        1018.89   1980.47   61.36   63.68   59.20    0.61\n",
      "  0     100         830.86   1606.56   69.56   72.37   66.95    0.70\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_trf_acc\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-07 13:10:12,075] [INFO] Set up nlp object from config\n",
      "[2023-03-07 13:10:12,092] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2023-03-07 13:10:12,096] [INFO] Created vocabulary\n",
      "[2023-03-07 13:10:12,098] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-03-07 13:10:28,593] [INFO] Initialized pipeline components: ['transformer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# train spacy\n",
    "!python -m spacy train E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_ner_trf_acc.cfg --output E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\models\\model_ner_trf_acc\\ --paths.train E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\data\\Train_ner\\train\\train.spacy --paths.dev E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\data\\Train_ner\\dev\\dev.spacy --training.eval_frequency 10 --training.max_steps 100 --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bac9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy debug config E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_2.cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146fb77",
   "metadata": {},
   "source": [
    "# Approaching as Span Categorizer problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f48bed",
   "metadata": {},
   "source": [
    "## SpanCat with transformer\n",
    "#### config_spancat_trf_acc is made for span categorization and accuracy with transformer backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1376a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m[+] Saved config\u001b[0m\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs_spancat\\config_spancat_trf_acc.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_spancat_trf_acc.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# auto fill the config_2 file\n",
    "!python -m spacy init fill-config E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_spancat_trf_acc.cfg E:\\Work\\Data_Science\\Projects\\Custom_NER_Spacy3.5\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_spancat_trf_acc.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f2a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory:\n",
      "E:\\Work\\Data_Science\\Projects\\Custom_NER\\models\\models_spancat_trf_acc\u001b[0m\n",
      "\u001b[38;5;4m[i] Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['transformer', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  -------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        7057.29       5380.86        0.60        0.30       56.69    0.01\n",
      "\u001b[38;5;3m[!] Aborting and saving the final best model. Encountered exception:\n",
      "OutOfMemoryError('CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 4.00\n",
      "GiB total capacity; 2.89 GiB already allocated; 0 bytes free; 3.05 GiB reserved\n",
      "in total by PyTorch) If reserved memory is >> allocated memory try setting\n",
      "max_split_size_mb to avoid fragmentation.  See documentation for Memory\n",
      "Management and PYTORCH_CUDA_ALLOC_CONF')\u001b[0m\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-09 20:58:47,044] [INFO] Set up nlp object from config\n",
      "[2023-03-09 20:58:47,062] [INFO] Pipeline: ['transformer', 'spancat']\n",
      "[2023-03-09 20:58:47,066] [INFO] Created vocabulary\n",
      "[2023-03-09 20:58:47,067] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-03-09 20:59:08,929] [INFO] Initialized pipeline components: ['transformer', 'spancat']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\cli\\_util.py\", line 74, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\cli\\train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\cli\\train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\training\\loop.py\", line 124, in train\n",
      "    raise e\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\training\\loop.py\", line 107, in train\n",
      "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\training\\loop.py\", line 209, in train_while_improving\n",
      "    nlp.update(\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy\\language.py\", line 1155, in update\n",
      "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy_transformers\\pipeline_component.py\", line 297, in update\n",
      "    trf_full, bp_trf_full = self.model.begin_update(docs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\thinc\\model.py\", line 309, in begin_update\n",
      "    return self._func(self, X, is_train=True)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\spacy_transformers\\layers\\transformer_model.py\", line 199, in forward\n",
      "    model_output, bp_tensors = transformer(wordpieces, is_train)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\thinc\\model.py\", line 291, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\thinc\\layers\\pytorchwrapper.py\", line 219, in forward\n",
      "    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\thinc\\shims\\pytorch.py\", line 90, in __call__\n",
      "    return self.begin_update(inputs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\thinc\\shims\\pytorch.py\", line 124, in begin_update\n",
      "    output = self._model(*inputs.args, **inputs.kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 851, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 526, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 453, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\pytorch_utils.py\", line 249, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 465, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 364, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\ritap\\anaconda3\\envs\\ner_spacy35\\lib\\site-packages\\transformers\\activations.py\", line 57, in forward\n",
      "    return self.act(input)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 4.00 GiB total capacity; 2.89 GiB already allocated; 0 bytes free; 3.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train spacy\n",
    "!python -m spacy train E:\\Work\\Data_Science\\Projects\\Custom_NER\\Custom_Entity_Extraction_Spacy3.5\\configs\\config_spancat_trf_acc.cfg --output E:\\Work\\Data_Science\\Projects\\Custom_NER\\models\\models_spancat_trf_acc\\ --paths.train E:\\Work\\Data_Science\\Projects\\Custom_NER\\data\\Train_spancat\\train\\train.spacy --paths.dev E:\\Work\\Data_Science\\Projects\\Custom_NER\\data\\Train_spancat\\dev\\dev.spacy --training.eval_frequency 10 --training.max_steps 100 --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44a95b",
   "metadata": {},
   "source": [
    "#### N.B. Due to lack of hardware resource the training could not be completed for 100 epochs, instead it could only complete first few epochs and saved the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d430e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30328c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbeced12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x20ea5511ba0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load(\"en_core_web_trf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4367033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52238f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
